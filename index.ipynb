{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoUY0aVqvPsr"
      },
      "source": [
        "# About\n",
        "\n",
        "The purpose of this document is to do the following:\n",
        "- describe a set of data pulled from a Dataverse\n",
        "- provide Python code and a simple computing environment for working with the data (without the need to manually build your own)\n",
        "- perform a set of checks and validation on the data\n",
        "\n",
        "**NOTE: The code and this environment may become deprecated at some point in time and may no longer function as originally designed. Also, the code and instructions included here are NOT the only way to work with data from our datasets NOR are they exhaustive examples of working with the Dataverse API. Only some of the most useful code examples are included, and as with any code, there are many ways to achieve similar results. For context, this notebook was last run on:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ObKF_lPsvPsu",
        "outputId": "92403a44-d711-413f-a28f-4570f8c4ab87",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-01 00:22:08.338921\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "currentDateAndTime = datetime.now()\n",
        "print(currentDateAndTime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgQePz9_vPsv"
      },
      "source": [
        "## Using this Jupyter notebook on data from the Dataverse\n",
        "\n",
        "This Jupyter notebook uses a Python environment, along with the Python `pandas` module, to retrieve STATA data from a Dataverse and copy it to a dataframe. Our Python environment is defined in the `environment.yml` configuration file, so if we needed some additional Python modules to build charts or graphs later, this is where we would add those modules. We chose STATA as the data source because the pandas module will automatically assign columns/variables as categorical if variable value labels are assigned in STATA. If we only downloaded the default .tab file from the Dataverse you would only have the raw data, which would require manually mapping the variable categories to the data.\n",
        "\n",
        "Note: If you are more familiar with using the R project to work data than Python, then you could modify this notebook to use the 'foreign' package for R [https://cran.r-project.org/web/packages/foreign/foreign.pdf], but we are not including every possible use-case in this document and are only using Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGCkVyLCvPsv",
        "tags": []
      },
      "source": [
        "## Prepare to pull data from the Dataverse\n",
        "\n",
        "The first you will need to retrieve your Dataverse API token which you can find at https://demo.dataverse.org/dataverseuser.xhtml?selectTab=apiTokenTab. This is simply to let the Dataverse that you are requesting the data and not someone else. Run the next block of code in the notebook and you will be asked to enter your API token. Simply copy the token from the link listed above and paste it into the text field asking for your token below. If you do not receive an error after entering your API token then you can move on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yObV-eLzr9AG",
        "outputId": "b7c249df-f727-4a0c-afaf-89b42d81c51b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "API_TOKEN = input(\"Enter your Dataverse API Token: \")     # need to have the user input their Dataverse API token since we do not want it stored in the notebook\n",
        "clear_output(wait=True) # we do not want the API token saved as output in the notebook so we clear it (processed on the next line)\n",
        "print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3gI39nqr9AG"
      },
      "source": [
        "## Retrieve the dataset from the Dataverse\n",
        "\n",
        "Now we can begin to retrieve the data and work with it. To do that we will run the following code to download a zip file of the entire dataset which includes the images (found in the `/data/images` directory). The zip file contains a `MANIFEST.TXT` file at the root of the archive which lists all of the files within the archive. The `MANIFEST.TXT` file is automatically created by the Dataverse software when the zip archive of a dataset is downloaded, so this file does not appear in the list of files of a dataset when simply browsing the Dataverse website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L5QzNKI6r9AG",
        "outputId": "993d8660-9800-4101-bd29-baadc2cc9fcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving the dataset from:  https://demo.dataverse.org/api/access/dataset/:persistentId/versions/8.0?format=original&persistentId=doi:10.70122/FK2/YUUS5M\n",
            "--2023-05-01 00:58:00--  https://demo.dataverse.org/api/access/dataset/:persistentId/versions/8.0?format=original&persistentId=doi:10.70122/FK2/YUUS5M\n",
            "Resolving demo.dataverse.org (demo.dataverse.org)... 34.194.176.74\n",
            "Connecting to demo.dataverse.org (demo.dataverse.org)|34.194.176.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/myDataset.zip’\n",
            "\n",
            "/tmp/myDataset.zip      [   <=>              ] 406.58K   854KB/s    in 0.5s    \n",
            "\n",
            "2023-05-01 00:58:01 (854 KB/s) - ‘/tmp/myDataset.zip’ saved [416339]\n",
            "\n",
            "List the contents of the archive:  ['data/image/', 'data/image/00b8a500-48c3-4961-938a-b1d04a78bf9e.png', 'code/', 'code/environment.yml', 'data/', 'data/f1_subject.sas7bdat', 'data/F1_SUBJECT.dta', 'data/F2_IMAGE_REF.dta', 'code/index.ipynb', 'MANIFEST.TXT']\n",
            "dataset image count: 1\n",
            "compare the list of image file references in df2 and compare to the contents of the zip archive\n"
          ]
        }
      ],
      "source": [
        "SERVER_URL=\"https://demo.dataverse.org\"\n",
        "PERSISTENT_DATASET_ID=\"doi:10.70122/FK2/YUUS5M\"\n",
        "DATASET_VERSION=\"8.0\"\n",
        "DATASET_URL = SERVER_URL + \"/api/access/dataset/:persistentId/versions/\"+ DATASET_VERSION + \"?format=original&persistentId=\" + PERSISTENT_DATASET_ID # include `format=original` so the STATA file will be include and not simply the tab delimited version\n",
        "DATASET_LOCAL_ZIP_PATH = '/tmp/myDataset.zip'   # where we will save the zip file on the virtual machine (VM)\n",
        "DATASET_LOCAL_ZIP_EXTRACTION_PATH = '/tmp/myDataset'\n",
        "print(\"Retrieving the dataset from: \", DATASET_URL)\n",
        "\n",
        "# we can use `wget` to retrieve the dataset zip file\n",
        "header='--header=X-Dataverse-key: {}'.format(API_TOKEN)\n",
        "!wget --no-check-certificate \\\n",
        "    \"$header\" \\\n",
        "    \"$DATASET_URL\" \\\n",
        "    -O \"$DATASET_LOCAL_ZIP_PATH\"\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "# save the zip file locally\n",
        "local_zip = DATASET_LOCAL_ZIP_PATH\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall(\"$DATASET_LOCAL_ZIP_EXTRACTION_PATH\")  # extract the zip file contents to our virual machine running this notebook\n",
        "\n",
        "print('List the contents of the archive: ', zip_ref.namelist()) # this should match the manifest.txt contents\n",
        "print('dataset image count:', len(os.listdir('$DATASET_LOCAL_ZIP_EXTRACTION_PATH/data/image')))\n",
        "\n",
        "zip_ref.close()   # close the zip file after we are finished working with it\n",
        "\n",
        "# we need to compare the list of image file references in df2 and compare to the contents of the zip archive\n",
        "print(\"compare the list of image file references in df2 and compare to the contents of the zip archive\")\n",
        "\n",
        "# we will ignore this since it is not useful\n",
        "# dfjoin = df2.join(df1, lsuffix='_df1', rsuffix='_df2')  # for information on joins visit https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html\n",
        "# print(dfjoin)\n",
        "\n",
        "# print(\"Start dataset join statistics\\n=======================================\")\n",
        "# print(dfjoin.head(3))  # print the first three rows of the dataframe, including the header (which might be wrapped to a second line)\n",
        "# print(dfjoin.describe())  # prints general stats for all vars (this is useful because it can easily point out data errors, such as duplicates in a unique field where the `count` may be reported as 628 but the `unique` may be showing 627)\n",
        "# print(\"=======================================\\nend dataset join statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the data pulled from the Dataverse we can begin reading and working with some of the data files."
      ],
      "metadata": {
        "id": "5Vxftxck2E3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # tell Python we want to use the pandas module\n",
        "DATA_FILE1 = 'data/F1_SUBJECT.dta'  # this first file contains some demographics\n",
        "df1 = pd.read_stata(\"$DATASET_LOCAL_ZIP_EXTRACTION_PATH\" + \"/\" + DATA_FILE1)  # read the subject data file and save it to a dataframe named `df1`"
      ],
      "metadata": {
        "id": "UP_8q0bXy-m1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAw0BpVrvPsw"
      },
      "source": [
        "## Print general stats from the STATA data\n",
        "\n",
        "Now we will print some general statistics on the data we just downloaded and saved to dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5_wQ-lHXvPsw",
        "outputId": "cff385c6-589e-492f-9995-e45843dd8f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start dataset 1 statistics\n",
            "=======================================\n",
            "                           E03SUBJECTID E03RADLPAKKL  E03RADRPAKKL E03AGE  \\\n",
            "0  e731b53a-c413-41bd-8018-14f3fb7cfb7b     Moderate      Moderate  50-54   \n",
            "1  86988f07-1c1d-42fc-b23b-6ef7b829a7e1          NaN          Mild  60-64   \n",
            "2  3b58c0d3-a33b-4a0c-b231-34985f7d23eb         Mild  Questionable  65-70   \n",
            "\n",
            "  E03GENDER  E03PASKL  E03PASKR  \n",
            "0    Female      None  Moderate  \n",
            "1    Female      None    Severe  \n",
            "2    Female  Moderate  Moderate  \n",
            "                                E03SUBJECTID E03RADLPAKKL  E03RADRPAKKL  \\\n",
            "count                                    628          618           616   \n",
            "unique                                   627            6             6   \n",
            "top     84882a65-ba24-42b2-bb29-0bd3ac001927        No OA  Questionable   \n",
            "freq                                       2          237           216   \n",
            "\n",
            "       E03AGE E03GENDER E03PASKL E03PASKR  \n",
            "count     628       628      627      627  \n",
            "unique      7         2        4        4  \n",
            "top     65-70    Female     None     None  \n",
            "freq      126       416      334      296  \n",
            "=======================================\n",
            "end dataset 1 statistics\n"
          ]
        }
      ],
      "source": [
        "print(\"Start dataset 1 statistics\\n=======================================\")\n",
        "print(df1.head(3))  # print the first three rows of the dataframe, including the header (which might be wrapped to a second line)\n",
        "# print(df1['E03RADRPAKKL'].dtype) # this should show the data type as categorical\n",
        "# print(df1[['E03RADRPAKKL','E03RADRPAKKL','E03AGE','E03GENDER']].describe())  # prints general stats for select variables\n",
        "print(df1.describe())  # prints general stats for all vars (this is useful because it can easily point out data errors, such as duplicates in a unique field where the `count` may be reported as 628 but the `unique` may be showing 627)\n",
        "print(\"=======================================\\nend dataset 1 statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHga8UutvPsx"
      },
      "source": [
        "## Data curation and performing automated data validation and checks\n",
        "\n",
        "It can be difficult to spot data issues unless you know what to look for. As a data curator, it is important to define the values you expect, or do not expect to see in a data column. For example, our `E03SUBJECTID` values should all be unique in our `df1` dataset, meaning the total number of values should equal the total number of unique values.\n",
        "\n",
        "In the stats above, if were had read the data carfully we might have spotted that our `E03SUBJECTID` statistics do not look correct with regard to the unique value count in the column. With that in mind we should create reproducible checks on our data to ensure the data meets criteria we define for our data. This is especially useful if you will be uploading new versions of the data to the Dataverse frequently since you do not want to manually check for the same issues on new data. Plus, having the validation defined in a file gives the end-user confidence that you are not simply manually checking your data (or not checking your data), and instead can verify the data meets specific conditions automatically themselves.\n",
        "\n",
        "Since we expect our `E03SUBJECTID` column to contain unique values, we create a simple check using the Python code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BJP3_fp3vPsx",
        "outputId": "ce23748e-eb82-47ad-ba54-fa0bb7a2aa34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of values for E03SUBJECTID:  628\n",
            "total number of distinct values for E03SUBJECTID:  627\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ec6368eb8006>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# we post an error if the E03SUBJECTID column does not contain distinct values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"E03SUBJECTID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"E03SUBJECTID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***ERROR: The E03SUBJECTID column contains a duplicate value***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#df1.groupby([\"E03GENDER\"])[\"E03RADLPAKKL\"].count()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ***ERROR: The E03SUBJECTID column contains a duplicate value***"
          ]
        }
      ],
      "source": [
        "# let us print our counts just so we have a visual on what the data looks like\n",
        "print(\"total number of values for E03SUBJECTID: \",df1[\"E03SUBJECTID\"].count())\n",
        "print(\"total number of distinct values for E03SUBJECTID: \",df1[\"E03SUBJECTID\"].unique().size)\n",
        "\n",
        "# we post an error if the E03SUBJECTID column does not contain distinct values\n",
        "if df1[\"E03SUBJECTID\"].count()!=df1[\"E03SUBJECTID\"].unique().size:\n",
        "    raise RuntimeError(\"***ERROR: The E03SUBJECTID column contains a duplicate value***\")\n",
        "    \n",
        "#df1.groupby([\"E03GENDER\"])[\"E03RADLPAKKL\"].count()\n",
        "\n",
        "# show some combined data (https://pandas.pydata.org/docs/getting_started/intro_tutorials/08_combine_dataframes.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwb6eo6OvPsy"
      },
      "source": [
        "As designed, the code throws an error saying that our `E03SUBJECTID` column contains a duplicate value. Now that we know our data does not pass inspection we can go back and correct the data and then rerun this notebook to verify the data passes inspection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGh6EPP9vPsy"
      },
      "source": [
        "## Joining data\n",
        "\n",
        "Now we will pull in another data file that we plan to join to the data we were just working with. This is another STATA file (as we will stop working with SAS data going forward). This data will contain duplicate `E03SUBJECTID` values unlike our first data file, however the `E03USIMGFN` values should be unique. As we did prior, we will confirm that `E03USIMGFN` does contain unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cwhoH9CvPsz",
        "outputId": "d9351cc7-b113-4921-8795-01cf35090728",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start dataset 2 STATA statistics\n",
            "=======================================\n",
            "                           E03SUBJECTID   \n",
            "0  e731b53a-c413-41bd-8018-14f3fb7cfb7b  \\\n",
            "1  e731b53a-c413-41bd-8018-14f3fb7cfb7b   \n",
            "2  e731b53a-c413-41bd-8018-14f3fb7cfb7b   \n",
            "\n",
            "                                 E03USIMGFN         E03USIMGBP E03BODYSIDE  \n",
            "0  049c9252-6b05-4746-b6b4-d1b5172728a9.png      SUPRAPAT LONG  Right side  \n",
            "1  ac2201c4-a17b-414d-ac54-52a4724da98f.png      SUPRAPAT LONG   Left side  \n",
            "2  41914520-7091-4f29-9c09-270d3ab01144.png  SUPRAPAT LONG CPD  Right side  \n",
            "                                E03SUBJECTID   \n",
            "count                                   8664  \\\n",
            "unique                                   632   \n",
            "top     aa901e5d-1ab4-4db1-adec-b2d88a3e815f   \n",
            "freq                                      22   \n",
            "\n",
            "                                      E03USIMGFN     E03USIMGBP E03BODYSIDE  \n",
            "count                                       8664           8664        8664  \n",
            "unique                                      8664              7           2  \n",
            "top     049c9252-6b05-4746-b6b4-d1b5172728a9.png  SUPRAPAT LONG  Right side  \n",
            "freq                                           1           1246        4332  \n",
            "=======================================\n",
            "end dataset 2 STATA statistics\n",
            "total number of values for E03USIMGFN:  8664\n",
            "total number of distinct values for E03USIMGFN:  8664\n"
          ]
        }
      ],
      "source": [
        "DATA_FILE2 = 'https://demo.dataverse.org/api/access/datafile/2063000?format=original&version=7.0' \n",
        "df2 = pd.read_stata(DATA_FILE2)\n",
        "\n",
        "print(\"Start dataset 2 STATA statistics\\n=======================================\")\n",
        "print(df2.head(3))  # print the first three rows of the dataframe, including the header (which might be wrapped to a second line)\n",
        "print(df2.describe())  # prints general stats for all vars (this is useful because it can easily point out data errors, such as duplicates in a unique field where the `count` may be reported as 628 but the `unique` may be showing 627)\n",
        "print(\"=======================================\\nend dataset 2 STATA statistics\")\n",
        "\n",
        "# let us print our counts just so we have a visual on what the data looks like\n",
        "print(\"total number of values for E03USIMGFN: \",df2[\"E03USIMGFN\"].count())\n",
        "print(\"total number of distinct values for E03USIMGFN: \",df2[\"E03USIMGFN\"].unique().size)\n",
        "\n",
        "# we post an error if the E03USIMGFN column does not contain distinct values\n",
        "if df2[\"E03USIMGFN\"].count()!=df2[\"E03USIMGFN\"].unique().size:\n",
        "    raise RuntimeError(\"***ERROR: The E03USIMGFN column contains a duplicate value***\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB7uf5Dtr9AH"
      },
      "source": [
        "## Working with the data (by NOT downloading the entire dataset) \n",
        "\n",
        "Below we will begin with some code we will need to work with our data. We need to specific the files we want to analyze from the Dataverse and import the Python module (pandas) that will help us describe and make sense of the data. No output will be printed from this code as we are simply retrieving the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od4Xi41mvPsw",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# pull the original STATA file at a specific version (using ?format=original&version=5.2) from the Dataverse, otherwise we would simply be pulling the CSV file without the categories included\n",
        "DATA_FILE1 = 'https://demo.dataverse.org/api/access/datafile/2062199?format=original&version=5.2'  # this first file contains some demographics; we want to be explicit on the data version and file format so everyone knows exactly which files are being used\n",
        "\n",
        "DATA_FILE1SAS = 'https://demo.dataverse.org/api/access/datafile/2062440?format=original&version=6.1' # this is a cleaner version of `DATA_FILE1` however it is stored in SAS format\n",
        "\n",
        "# see https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html on how to group and describe data\n",
        "import pandas as pd  # tell Python we want to use the pandas module\n",
        "df1 = pd.read_stata(DATA_FILE1)  # read the data file from the URL defined in the `DATA_FILE1` variable and save it to a dataframe named `df1`\n",
        "df1sas = pd.read_sas(DATA_FILE1SAS, format='sas7bdat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wquJF994vPsy"
      },
      "source": [
        "## Comparing STATA and SAS data\n",
        "\n",
        "Now we will print statistics on the SAS data we just downloaded and saved to a dataframe. This SAS data file does not contain \n",
        "the duplicate values we had in our test STATA data file.\n",
        "\n",
        "Spoiler alert, pandas heavily favors the STATA file format and as such does not support SAS formats (variable labels), thus we will only see the raw data and no variable labels. With that said, it makes it much more difficult to justify working with the SAS data within this notebook. Also, you will see that the stats are using float values, which we do not want. So stick with STATA for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTG3M7s7vPsy",
        "outputId": "b45e5cae-2828-415c-8429-0841945f5e30",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start dataset 1 SAS statistics\n",
            "=======================================\n",
            "                              E03SUBJECTID  E03RADLPAKKL  E03RADRPAKKL   \n",
            "0  b'0053be32-a7d2-4216-84be-e725308c3569'           0.0           1.0  \\\n",
            "1  b'0060b8e1-97a8-4c46-91ed-ea20291e50b9'           3.0           4.0   \n",
            "2  b'00b245a4-4b03-410d-8219-6d37a67817d1'           0.0           1.0   \n",
            "\n",
            "   E03AGE  E03GENDER  E03PASKL  E03PASKR  \n",
            "0     4.0        1.0       0.0       0.0  \n",
            "1     6.0        2.0       2.0       0.0  \n",
            "2     4.0        2.0       0.0       0.0  \n",
            "       E03RADLPAKKL  E03RADRPAKKL      E03AGE   E03GENDER    E03PASKL   \n",
            "count    627.000000    627.000000  627.000000  627.000000  627.000000  \\\n",
            "mean       1.261563      1.314195    4.580542    1.661882    0.846890   \n",
            "std        4.094160      4.097670    1.859559    0.473447    1.049343   \n",
            "min       -1.000000     -1.000000    1.000000    1.000000   -1.000000   \n",
            "25%        0.000000      0.000000    3.000000    1.000000    0.000000   \n",
            "50%        1.000000      1.000000    5.000000    2.000000    0.000000   \n",
            "75%        2.000000      2.000000    6.000000    2.000000    2.000000   \n",
            "max       99.000000     99.000000    7.000000    2.000000    3.000000   \n",
            "\n",
            "         E03PASKR  \n",
            "count  627.000000  \n",
            "mean     0.937799  \n",
            "std      1.054864  \n",
            "min     -1.000000  \n",
            "25%      0.000000  \n",
            "50%      1.000000  \n",
            "75%      2.000000  \n",
            "max      3.000000  \n",
            "=======================================\n",
            "end dataset 1 SAS statistics\n",
            "total number of values for E03SUBJECTID:  627\n",
            "total number of distinct values for E03SUBJECTID:  627\n"
          ]
        }
      ],
      "source": [
        "print(\"Start dataset 1 SAS statistics\\n=======================================\")\n",
        "print(df1sas.head(3))  # print the first three rows of the dataframe, including the header (which might be wrapped to a second line)\n",
        "# print(df1sas['E03RADRPAKKL'].dtype) # this should show the data type as categorical\n",
        "# print(df1sas[['E03RADRPAKKL','E03RADRPAKKL','E03AGE','E03GENDER']].describe())  # prints general stats for select variables\n",
        "print(df1sas.describe())  # prints general stats for all vars (this is useful because it can easily point out data errors, such as duplicates in a unique field where the `count` may be reported as 628 but the `unique` may be showing 627)\n",
        "print(\"=======================================\\nend dataset 1 SAS statistics\")\n",
        "\n",
        "# let us print our counts just so we have a visual on what the data looks like\n",
        "print(\"total number of values for E03SUBJECTID: \",df1sas[\"E03SUBJECTID\"].count())\n",
        "print(\"total number of distinct values for E03SUBJECTID: \",df1sas[\"E03SUBJECTID\"].unique().size)\n",
        "\n",
        "# we post an error if the E03SUBJECTID column does not contain distinct values\n",
        "if df1sas[\"E03SUBJECTID\"].count()!=df1sas[\"E03SUBJECTID\"].unique().size:\n",
        "    raise RuntimeError(\"***ERROR: The SAS E03SUBJECTID column contains a duplicate value***\")\n",
        "    \n",
        "#df1sas.groupby([\"E03GENDER\"])[\"E03RADLPAKKL\"].count()\n",
        "\n",
        "# show some combined data (https://pandas.pydata.org/docs/getting_started/intro_tutorials/08_combine_dataframes.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}